
import torch
import torch.nn as nn
from transformers import BertTokenizer, BertModel
from torch.nn.utils.rnn import pad_sequence
import json
from tqdm import tqdm
import logging
import time
from sklearn.metrics import precision_recall_fscore_support
import itertools

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# Load BERT model and tokenizer
logger.info("Loading BERT model and tokenizer...")
start_time = time.time()
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased').to(device)
logger.info(f"BERT loaded in {time.time() - start_time:.2f} seconds")

# Add [EMPTY] token to tokenizer
if '[EMPTY]' not in tokenizer.get_vocab():
    tokenizer.add_tokens(['[EMPTY]'])
    bert_model.resize_token_embeddings(len(tokenizer))

# Dual LSTM Model with Attention
class DualLSTMDeceptionDetector(nn.Module):
    def __init__(self, num_countries, country_emb_dim, hidden_dim, bert_dim=768):
        super(DualLSTMDeceptionDetector, self).__init__()
        self.country_embedding = nn.Embedding(num_countries, country_emb_dim)
        self.sender_lstm = nn.LSTM(country_emb_dim + bert_dim, hidden_dim, batch_first=True)
        self.receiver_lstm = nn.LSTM(country_emb_dim + bert_dim, hidden_dim, batch_first=True)
        # Attention layer
        self.attention = nn.Linear(hidden_dim * 2, 1)  # Concatenated sender and receiver hidden states
        self.sender_classifier = nn.Linear(hidden_dim, 1)
        self.receiver_classifier = nn.Linear(hidden_dim, 1)
        self.combined_classifier = nn.Linear(hidden_dim, 1)

    def forward(self, messages, senders, receivers):
        batch_size, seq_len = senders.shape
        message_embs = self.get_message_embeddings(messages, batch_size, seq_len)
        
        # Prepare inputs
        sender_embs = self.country_embedding(senders)  # Expects senders as torch.long
        receiver_embs = self.country_embedding(receivers)
        sender_input = torch.cat([message_embs, sender_embs], dim=2)
        receiver_input = torch.cat([message_embs, receiver_embs], dim=2)
        
        # LSTM outputs
        sender_out, _ = self.sender_lstm(sender_input)  # (batch_size, seq_len, hidden_dim)
        receiver_out, _ = self.receiver_lstm(receiver_input)  # (batch_size, seq_len, hidden_dim)
        
        # Attention: sender attends to receiver
        combined = torch.cat([sender_out, receiver_out], dim=2)  # (batch_size, seq_len, hidden_dim * 2)
        attention_scores = self.attention(combined).squeeze(-1)  # (batch_size, seq_len)
        attention_weights = torch.softmax(attention_scores, dim=1).unsqueeze(2)  # (batch_size, seq_len, 1)
        attended = (sender_out * attention_weights).sum(dim=1)  # (batch_size, hidden_dim)
        
        # Predictions
        sender_pred = self.sender_classifier(sender_out)  # (batch_size, seq_len, 1)
        receiver_pred = self.receiver_classifier(receiver_out)  # (batch_size, seq_len, 1)
        final_pred = self.combined_classifier(attended.unsqueeze(1).repeat(1, seq_len, 1))  # (batch_size, seq_len, 1)
        
        return sender_pred, receiver_pred, final_pred

    def get_message_embeddings(self, messages, batch_size, seq_len):
        message_embs = []
        for batch_idx in range(batch_size):
            seq_embs = []
            for seq_idx in range(seq_len):
                if seq_idx < len(messages[batch_idx]):
                    msg = messages[batch_idx][seq_idx] or '[EMPTY]'  # Handle empty messages
                    inputs = tokenizer(msg, return_tensors='pt', padding=True, truncation=True, max_length=128)
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    with torch.no_grad():
                        outputs = bert_model(**inputs).last_hidden_state[:, 0, :]  # CLS token
                    seq_embs.append(outputs.squeeze(0))
                else:
                    seq_embs.append(torch.zeros(768, device=device))
            message_embs.append(torch.stack(seq_embs))
        return torch.stack(message_embs)

# Data Preprocessing
def preprocess_data(data_file, country_map=None):
    logger.info(f"Preprocessing {data_file}")
    with open(data_file, 'r') as f:
        data = [json.loads(line) for line in f]
    
    if country_map is None:
        countries = set()
        for game in data:
            countries.update(game['speakers'])
            countries.update(game['receivers'])
        country_map = {c: i for i, c in enumerate(sorted(countries))}
        logger.info(f"Created country map with {len(country_map)} countries")
    
    conversations = []
    for game in data:
        messages = [msg if msg else '[EMPTY]' for msg in game['messages']]
        senders = [country_map[s] for s in game['speakers']]
        receivers = [country_map[r] for r in game['receivers']]
        sender_labels = [1 if l else 0 for l in game['sender_labels']]  # True -> 1, False -> 0
        receiver_labels = []
        for l in game['receiver_labels']:
            if l == "NOANNOTATION":
                receiver_labels.append(-1)
            else:
                receiver_labels.append(1 if l else 0)
        
        conversations.append({
            'messages': messages,
            'speakers': torch.tensor(senders, dtype=torch.long),  # Explicitly set to long
            'receivers': torch.tensor(receivers, dtype=torch.long),  # Explicitly set to long
            'sender_labels': torch.tensor(sender_labels, dtype=torch.float32),
            'receiver_labels': torch.tensor(receiver_labels, dtype=torch.float32)
        })
    return conversations, country_map

# Compute Metrics
def compute_metrics(preds, labels, mask=None):
    if mask is not None:
        preds = preds[mask.bool()]
        labels = labels[mask.bool()]
    preds = (preds > 0.5).float()  # Threshold at 0.5
    labels = labels.float()
    
    precision, recall, fscore, _ = precision_recall_fscore_support(labels.cpu(), preds.cpu(), average=None, labels=[0, 1], zero_division=0)
    micro = precision_recall_fscore_support(labels.cpu(), preds.cpu(), average='micro', zero_division=0)[:3]
    macro = precision_recall_fscore_support(labels.cpu(), preds.cpu(), average='macro', zero_division=0)[:3]
    
    return {
        'True_precision': precision[1],
        'False_precision': precision[0],
        'True_recall': recall[1],
        'False_recall': recall[0],
        'True_fscore': fscore[1],
        'False_fscore': fscore[0],
        'micro_precision': micro[0],
        'micro_recall': micro[1],
        'micro_fscore': micro[2],
        'macro_precision': macro[0],
        'macro_recall': macro[1],
        'macro_fscore': macro[2]
    }

# Training and Evaluation Function
def train_and_evaluate(model, train_data, val_data, test_data, epochs=5, batch_size=4):
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.BCEWithLogitsLoss(reduction='none')
    
    # Grid search for loss weights
    weight_combinations = list(itertools.product([0.2, 0.3, 0.4], repeat=3))
    weight_combinations = [w for w in weight_combinations if abs(sum(w) - 1.0) < 1e-6]
    logger.info(f"Tuning over {len(weight_combinations)} weight combinations: {weight_combinations}")
    
    best_val_loss = float('inf')
    best_weights = None
    best_metrics = None
    best_model_state = None
    
    for weights in weight_combinations:
        logger.info(f"Training with weights: {weights}")
        model.train()
        for epoch in range(epochs):
            total_loss = 0
            for i in tqdm(range(0, len(train_data), batch_size), desc=f"Epoch {epoch+1}/{epochs}"):
                batch = train_data[i:i+batch_size]
                messages = [c['messages'] for c in batch]
                senders = pad_sequence([c['speakers'] for c in batch], batch_first=True, padding_value=0).to(device)
                receivers = pad_sequence([c['receivers'] for c in batch], batch_first=True, padding_value=0).to(device)
                sender_labels = pad_sequence([c['sender_labels'] for c in batch], batch_first=True, padding_value=-1).to(device)
                receiver_labels = pad_sequence([c['receiver_labels'] for c in batch], batch_first=True, padding_value=-1).to(device)
                
                optimizer.zero_grad()
                try:
                    sender_pred, receiver_pred, final_pred = model(messages, senders, receivers)
                    
                    # Compute losses
                    loss_sender = criterion(sender_pred.squeeze(-1), sender_labels)
                    mask = (receiver_labels != -1)
                    loss_receiver = (criterion(receiver_pred.squeeze(-1), receiver_labels) * mask).sum() / (mask.sum() + 1e-8)
                    loss_combined = criterion(final_pred.squeeze(-1), sender_labels)
                    
                    # Mask sender and combined losses for padding
                    sender_mask = (sender_labels != -1)
                    loss_sender = (loss_sender * sender_mask).sum() / (sender_mask.sum() + 1e-8)
                    loss_combined = (loss_combined * sender_mask).sum() / (sender_mask.sum() + 1e-8)
                    
                    loss = weights[0] * loss_sender + weights[1] * loss_receiver + weights[2] * loss_combined
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                except Exception as e:
                    logger.error(f"Error in batch {i}: {str(e)}")
                    continue
            
            avg_train_loss = total_loss / (len(train_data) / batch_size) if len(train_data) > 0 else float('inf')
            
            # Validation
            model.eval()
            val_loss = 0
            all_preds, all_labels = [], []
            with torch.no_grad():
                for i in range(0, len(val_data), batch_size):
                    batch = val_data[i:i+batch_size]
                    messages = [c['messages'] for c in batch]
                    senders = pad_sequence([c['speakers'] for c in batch], batch_first=True, padding_value=0).to(device)
                    receivers = pad_sequence([c['receivers'] for c in batch], batch_first=True, padding_value=0).to(device)
                    sender_labels = pad_sequence([c['sender_labels'] for c in batch], batch_first=True, padding_value=-1).to(device)
                    receiver_labels = pad_sequence([c['receiver_labels'] for c in batch], batch_first=True, padding_value=-1).to(device)
                    
                    _, _, final_pred = model(messages, senders, receivers)
                    loss_combined = criterion(final_pred.squeeze(-1), sender_labels)
                    mask = (sender_labels != -1)
                    loss_combined = (loss_combined * mask).sum() / (mask.sum() + 1e-8)
                    val_loss += loss_combined.item()
                    
                    all_preds.append(final_pred.squeeze(-1)[mask])
                    all_labels.append(sender_labels[mask])
            
            avg_val_loss = val_loss / (len(val_data) / batch_size) if len(val_data) > 0 else float('inf')
            all_preds = torch.cat(all_preds)
            all_labels = torch.cat(all_labels)
            metrics = compute_metrics(all_preds, all_labels)
            
            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                best_weights = weights
                best_metrics = metrics
                best_model_state = model.state_dict().copy()
            
            logger.info(f"Epoch {epoch+1}/{epochs} - Weights {weights} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")
    
    # Load best model
    model.load_state_dict(best_model_state)
    logger.info(f"Best weights selected: {best_weights} with Val Loss: {best_val_loss:.4f}")
    
    # Test Evaluation
    model.eval()
    test_loss = 0
    all_preds, all_labels = [], []
    with torch.no_grad():
        for i in range(0, len(test_data), batch_size):
            batch = test_data[i:i+batch_size]
            messages = [c['messages'] for c in batch]
            senders = pad_sequence([c['speakers'] for c in batch], batch_first=True, padding_value=0).to(device)
            receivers = pad_sequence([c['receivers'] for c in batch], batch_first=True, padding_value=0).to(device)
            sender_labels = pad_sequence([c['sender_labels'] for c in batch], batch_first=True, padding_value=-1).to(device)
            receiver_labels = pad_sequence([c['receiver_labels'] for c in batch], batch_first=True, padding_value=-1).to(device)
            
            _, _, final_pred = model(messages, senders, receivers)
            loss_combined = criterion(final_pred.squeeze(-1), sender_labels)
            mask = (sender_labels != -1)
            loss_combined = (loss_combined * mask).sum() / (mask.sum() + 1e-8)
            test_loss += loss_combined.item()
            
            all_preds.append(final_pred.squeeze(-1)[mask])
            all_labels.append(sender_labels[mask])
    
    avg_test_loss = test_loss / (len(test_data) / batch_size) if len(test_data) > 0 else float('inf')
    all_preds = torch.cat(all_preds)
    all_labels = torch.cat(all_labels)
    test_metrics = compute_metrics(all_preds, all_labels)
    
    return {
        'training_loss': avg_train_loss,
        'validation_loss': avg_val_loss,
        'best_validation_loss': best_val_loss,
        'test_loss': avg_test_loss,
        'best_validation_metrics': best_metrics,
        'test_metrics': test_metrics
    }

# Main Execution
if __name__ == "__main__":
    # Replace with your actual file paths
    train_file = "/kaggle/input/dataset-deception/train.jsonl"
    val_file = "/kaggle/input/dataset-deception/validation.jsonl"
    test_file = "/kaggle/input/dataset-deception/test.jsonl"
    
    # Preprocess data
    train_data, country_map = preprocess_data(train_file)
    val_data, _ = preprocess_data(val_file, country_map)
    test_data, _ = preprocess_data(test_file, country_map)
    
    # Initialize model
    num_countries = len(country_map)
    model = DualLSTMDeceptionDetector(num_countries=num_countries, country_emb_dim=16, hidden_dim=128).to(device)
    logger.info(f"Model initialized with {num_countries} countries")
    
    # Train and evaluate
    results = train_and_evaluate(model, train_data, val_data, test_data, epochs=2, batch_size=4)
    
    # Print metrics in the requested format
    print("\n=== Per-Class Metrics (for 'True' and 'False' classes) ===")
    print("Precision")
    print(f"True_precision: {results['test_metrics']['True_precision']:.4f}")
    print(f"False_precision: {results['test_metrics']['False_precision']:.4f}")
    print("Recall")
    print(f"True_recall: {results['test_metrics']['True_recall']:.4f}")
    print(f"False_recall: {results['test_metrics']['False_recall']:.4f}")
    print("F1-Score")
    print(f"True_fscore: {results['test_metrics']['True_fscore']:.4f}")
    print(f"False_fscore: {results['test_metrics']['False_fscore']:.4f}")
    
    print("\n Micro-Averaged Metrics")
    print(f"micro_precision: {results['test_metrics']['micro_precision']:.4f}")
    print(f"micro_recall: {results['test_metrics']['micro_recall']:.4f}")
    print(f"micro_fscore: {results['test_metrics']['micro_fscore']:.4f}")
    
    print("\nMacro-Averaged Metrics")
    print(f"macro_precision: {results['test_metrics']['macro_precision']:.4f}")
    print(f"macro_recall: {results['test_metrics']['macro_recall']:.4f}")
    print(f"macro_fscore: {results['test_metrics']['macro_fscore']:.4f}")
    
    print("\n Loss")
    print(f"training_loss: {results['training_loss']:.4f}")
    print(f"validation_loss: {results['validation_loss']:.4f}")
    print(f"best_validation_loss: {results['best_validation_loss']:.4f}")
    print(f"test_loss: {results['test_loss']:.4f}")
    
    print("\n Best Validation Metrics (peak performance during training)")
    for metric in ['True_precision', 'False_precision', 'True_recall', 'False_recall', 
                   'True_fscore', 'False_fscore', 'micro_precision', 'micro_recall', 
                   'micro_fscore', 'macro_precision', 'macro_recall', 'macro_fscore']:
        print(f"best_validation_{metric}: {results['best_validation_metrics'][metric]:.4f}")


ye dual lSTM h handle isme class imbalance
